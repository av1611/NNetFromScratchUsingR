# and lots of unique products:
nlevels(sales$Prod)
# We check if Quant and Val are both missing
# together much (length returns number 888)
length(which(is.na(sales$Quant) & is.na(sales$Val)))
# With large data sets, is likely easier to
# simply perform "logical arithmetic" where
# TRUE=1 and FALSE=0:
sum(is.na(sales$Quant) & is.na(sales$Val))
# From summary() results, look at distribution
# of values in inspection column...proportion
# of frauds is relatively low, even if we only
# take into account the reports that were
# inspected (about 0.003166)
table(sales$Insp)/nrow(sales)*100
plot(table(sales$Insp)/nrow(sales)*100)
plot(table(log10(sales$Insp)/nrow(sales)*100))
(totS <- table(sales$ID))
(totP <- table(sales$Prod))
barplot(totS,
main='Transactions per salesperson',
names.arg='',xlab='Salespeople',
ylab='Amount')
barplot(log10(totS),
main='Transactions per salesperson',
names.arg='',xlab='Salespeople',
ylab='Amount')
barplot(totP,
main='Transactions per product',
names.arg='',xlab='Products',
ylab='Amount')
barplot(log10(totP),
main='Transactions per product',
names.arg='',xlab='Products',
ylab='Amount')
sales$Uprice <- sales$Val/sales$Quant
# When analyzing transactions over a short
# period of time, one does not expect strong
# variations of the unit price of the
# products.
# We check the distribution of the unit price:
summary(sales$Uprice)
upp <- aggregate(Uprice,list(Prod),median,na.rm=T)
# Let's take a look at upp
upp[1:10,]
attach(sales)
upp <- aggregate(Uprice,list(Prod),median,na.rm=T)
# Let's take a look at upp
upp[1:10,]
topP <- sapply(c(T, F), function(o) {
upp[order(upp[ , 2],
decreasing = o)[1 : 5], 1] } )
colnames(topP) <- c('Expensive', 'Cheap')
colnames(topP) <- c('Expensive', 'Cheap')
topP
tops <- sales[Prod %in% topP[1,],
c('Prod','Uprice')]
tops$Prod <- factor(tops$Prod)
# The scales of the prices of the most expensive
# and least expensive products are rather different.
# So we use a log scale to keep the values of
# the cheap product from being indistinguishable.
# Y-axis is on loag scale.
boxplot(Uprice ~ Prod,data=tops,
ylab='Uprice',log="y")
# We carry out a similar analysis to discover
# whcih salespeople are ones who bring more
# (less) money into the company.
vs <- aggregate(Val,list(ID),sum,na.rm=T)
scoresSs <- sapply(c(T,F),function(o)
vs[order(vs$x,decreasing=o)[1:5],1])
colnames(scoresSs) <- c('Most','Least')
scoresSs
sum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(Val,na.rm=T)*100
ystem.time({
want = which(rowSums(df) > 4.0)
output = rep("less than 4", times = nrow(df))
output[want] = "greater than 4"
})
system.time({
want = which(rowSums(df) > 4.0)
output = rep("less than 4", times = nrow(df))
output[want] = "greater than 4"
})
col1 <- runif (12^3, 0, 2)
col2 <- rnorm (12^3, 0, 2)
col3 <- rpois (12^3, 3)
col4 <- rchisq (12^3, 2)
df <- data.frame (col1, col2, col3, col4)
system.time({
for (i in 1:nrow(df)) {
if ((df[i, 'col1'] +
df[i, 'col2'] +
df[i, 'col3'] +
df[i, 'col4']) > 4) { # check if > 4
df[i, 5] <- "greater_than_4" # assign 5th column
} else {
df[i, 5] <- "lesser_than_4" # assign 5th column
}
}
})
output <- character (nrow(df)) # initialize output vector
system.time({
for (i in 1:nrow(df)) {
if ((df[i, 'col1'] +
df[i, 'col2'] +
df[i, 'col3'] +
df[i, 'col4']) > 4) {
output[i] <- "greater_than_4"
} else {
output[i] <- "lesser_than_4"
}
}
df$output
})
output <- character (nrow(df))
condition <-    (df$col1 +
df$col2 +
df$col3 +
df$col4) > 4  # condition check outside the loop
system.time({
for (i in 1:nrow(df)) {
if (condition[i]) {
output[i] <- "greater_than_4"
} else {
output[i] <- "lesser_than_4"
}
}
df$output <- output
})
output <- character(nrow(df))
condition <-   (df$col1 +
df$col2 +
df$col3 +
df$col4) > 4
system.time({
for (i in (1:nrow(df))[condition]) {  # run loop only for true conditions
if (condition[i]) {
output[i] <- "greater_than_4"
} else {
output[i] <- "lesser_than_4"
}
}
df$output
})
system.time({
output <- ifelse((df$col1 + df$col2 + df$col3 + df$col4) > 4,
"greater_than_4", "lesser_than_4")
df$output <- output
})
system.time({
want = which(rowSums(df) > 4.0)
output = rep("less than 4", times = nrow(df))
output[want] = "greater than 4"
})
system.time({
myfunc <- function(x) {
if ((x['col1'] + x['col2'] + x['col3'] + x['col4']) > 4) {
"greater_than_4"
}
else { "lesser_than_4" }
}
output <- apply(df[, c(1:4)], 1, FUN=myfunc)  # apply 'myfunc' on every row
df$output <- output
})
library(compiler)
myFuncCmp <- cmpfun(myfunc)
system.time({ output <- apply(df[, c (1:4)], 1, FUN=myFuncCmp) } )
library(foreach)
library(doSNOW)
cl <- makeCluster(4, type="SOCK") # for 4 cores machine
registerDoSNOW (cl)
condition <- (df$col1 + df$col2 + df$col3 + df$col4) > 4
# parallelization with vectorization
system.time({
output <- foreach(i = 1:nrow(df), .combine=c) %dopar% {
if (condition[i]) {
return("greater_than_4")
} else {
return("lesser_than_4")
}
}
})
df$output <- output
library(foreach)
library(doSNOW)
install.packages("doSNOW")
library(doSNOW)
cl <- makeCluster(4, type="SOCK") # for 4 cores machine
registerDoSNOW (cl)
condition <- (df$col1 + df$col2 + df$col3 + df$col4) > 4
# parallelization with vectorization
system.time({
output <- foreach(i = 1:nrow(df), .combine=c) %dopar% {
if (condition[i]) {
return("greater_than_4")
} else {
return("lesser_than_4")
}
}
})
df$output <- output
dt <- data.table(df)  # create the data.table
system.time({
for (i in 1:nrow (dt)) {
if ((dt[i, col1] +
dt[i, col2] +
dt[i, col3] +
dt[i, col4]) > 4) {
dt[i, col5:="greater_than_4"]  # assign the output as 5th column
} else {
dt[i, col5:="lesser_than_4"]  # assign the output as 5th column
}
}
})
library(caret)
library(C50)
library(ROCR)
library(caret)
library(plyr)
library(ROCR)
install.packages("C50")
library(C50)
library(doMC)
install.packages("doMC")
library(doMC)
registerDoMC(cores = 4)
# Splitting data into training and validation
# The following code splits 60% of data into training and remaining into validation.
trainIndex <- createDataPartition(data[ , 1], p = .6, list = FALSE, times = 1)
?createDataPartition
dev <- data[trainIndex, ]
val  <- data[-trainIndex, ]
trainIndex <- createDataPartition(data[ , 1], p = .6, list = FALSE, times = 1)
data("BJsales")
BJsales
trainIndex <- createDataPartition(BJsales[ , 1], p = .6, list = FALSE, times = 1)
View(BJsales)
data("baseball")
View("baseball")
View(baseball)
trainIndex <- createDataPartition(baseball[ , 1], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
val  <- data[-trainIndex, ]
baseball[, 3]
baseball[, 4]
trainIndex <- createDataPartition(baseball[ , 4
], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(baseball[ , 5], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(baseball[ , 6], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(baseball[ , 10], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex
trainIndex <- createDataPartition(baseball, p = .6, list = FALSE, times = 1)
data(GermanCredit)
dView(GermanCredit)
View(GermanCredit)
trainIndex <- createDataPartition(GermanCredit[ , 2], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(GermanCredit, p = .6, list = FALSE, times = 1)
trainIndex <- createDataPartition(GermanCredit[ , 1], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
View(GermanCredit)
View(GermanCredit)
trainIndex <- createDataPartition(GermanCredit[ , 8], p = .6, list = FALSE, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(GermanCredit[ , 8], p = .6, list = T, times = 1)
dev <- data[trainIndex, ]
x <- rgamma(50, 3, .5)
trainIndex <- createDataPartition(x, p = .6, list = F, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(x, p = .6, list = T, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(x, p = .6, list = T)
dev <- data[trainIndex, ]
val  <- data[-trainIndex, ]
trainIndex
x <- rgamma(50000, 3, .5)
trainIndex <- createDataPartition(x, p = .6, list = T)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(x, p = .6, times = 1)
dev <- data[trainIndex, ]
F
x <- rgamma(50, 3, .5)
trainIndex <- createDataPartition(x, p = .6, list = F, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(GermanCredit[, 1], p = .6, list = F, times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(iris$Species, p = .8,
list = FALSE,
times = 1)
dev <- data[trainIndex, ]
trainIndex <- createDataPartition(GermanCredit[, 1], p = .6, list = F, times = 1)
trainIndex <- createDataPartition(GermanCredit[, 1], p = .6, list = F, times = 1)
dev <- GermanCredit[trainIndex, ]
val  <- GermanCredit[-trainIndex, ]
cvCtrl <- trainControl(method = "repeatedcv",
number =10,
repeats =3,
classProbs = TRUE)
grid = expand.grid( .interaction.depth = seq(1, 7, by = 2),
.n.trees = seq(100, 1000, by = 50),
.shrinkage = c(0.01, 0.1))
# Example 1 : train with tuneGrid (Manual Grid)
grid <- expand.grid(.model = "tree",
.trials = c(1:100),
.winnow = FALSE)
set.seed(825)
tuned <- train(dev[, -1], dev[,1],
method = "C5.0",
metric = "ROC",
tuneGrid = grid,
trControl = cvCtrl)
set.seed(825)
tuned <- train(dev[ , -1], dev[ , 1],
method = "C5.0",
metric = "ROC",
tunelength = 10,
trControl = cvCtrl)
install.packages("funModeling")
library("funModeling")
data("heart_disease")
my_data_status=df_status(heart_disease)
View(my_data_status)
?funModeling
?funModeling::get_sample
library(ggplot2)
library(caret)
N <- 200 # number of points per class
D <- 2 # dimensionality
K <- 4 # number of classes
X <- data.frame() # data matrix (each row = single example)
y <- data.frame() # class labels
set.seed(308)
for (j in (1:K)) {
r <- seq(0.05,1,length.out = N) # radius
t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t))
ytemp <- data.frame(matrix(j, N, 1))
X <- rbind(X, Xtemp)
y <- rbind(y, ytemp)
}
data <- cbind(X,y)colnames(data) <- c(colnames(X), 'label')
colnames(data) <- c(colnames(X), 'label')
data <- cbind(X,y)
colnames(data) <- c(colnames(X), 'label')
x_min <- min(X[,1])-0.2;
x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2;
y_max <- max(X[,2])+0.2
ggplot(data) + geom_point(aes(x=x, y=y, color = as.character(label)), size = 2) +
theme_bw(base_size = 15) +
xlim(x_min, x_max) +
ylim(y_min, y_max) +
ggtitle('Spiral Data Visulization') +
coord_fixed(ratio = 0.8) +
theme(axis.ticks=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_blank(),
axis.title=element_blank(),
legend.position = 'none')
X <- as.matrix(X)
Y <- matrix(0, N*K, K)
for (i in 1:(N*K)){
Y[i, y[i,]] <- 1
}
nnet <- function(X, Y, step_size = 0.5, reg = 0.001, h = 10, niteration) {
# get dim of input
N <- nrow(X) # number of examples
K <- ncol(Y) # number of classes
D <- ncol(X) # dimensionality
# initialize parameters randomly
W <- 0.01 * matrix(rnorm(D*h), nrow = D)
b <- matrix(0, nrow = 1, ncol = h)
W2 <- 0.01 * matrix(rnorm(h*K), nrow = h)
b2 <- matrix(0, nrow = 1, ncol = K)
# gradient descent loop to update weight and bias
for (i in 0:niteration) {
# hidden layer, ReLU activation
hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T))
hidden_layer <- matrix(hidden_layer, nrow = N)
# class score
scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T)
# compute and normalize class probabilities
exp_scores <- exp(scores)
probs <- exp_scores / rowSums(exp_scores)
# compute the loss: sofmax and regularization
corect_logprobs <- -log(probs)
data_loss <- sum(corect_logprobs*Y)/N
reg_loss <- 0.5*reg*sum(W*W) + 0.5*reg*sum(W2*W2)
loss <- data_loss + reg_loss    # check progress
if (i%%1000 == 0 | i == niteration) { print(paste("iteration", i,': loss', loss))  }
# compute the gradient on scores
dscores <- probs-Y
dscores <- dscores/N
# backpropate the gradient to the parameters
dW2 <- t(hidden_layer) %*% dscores
db2 <- colSums(dscores)
# next backprop into hidden layer
dhidden <- dscores%*%t(W2)
# backprop the ReLU non-linearity
dhidden[hidden_layer <= 0] <- 0
# finally into W,b
dW <- t(X) %*% dhidden
db <- colSums(dhidden)
# add regularization gradient contribution
dW2 <- dW2 + reg *W2
dW <- dW + reg *W
# update parameter
W <- W-step_size*dW
b <- b-step_size*db
W2 <- W2-step_size*dW2
b2 <- b2-step_size*db2
}
return(list(W, b, W2, b2))
}
nnetPred <- function(X, para = list()) {
W <- para[[1]]
b <- para[[2]]
W2 <- para[[3]]
b2 <- para[[4]]
N <- nrow(X)
hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T))
hidden_layer <- matrix(hidden_layer, nrow = N)
scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T)
predicted_class <- apply(scores, 1, which.max)
return(predicted_class)
}
nnet.model <- nnet(X, Y, step_size = 0.4,reg = 0.0002, h=50, niteration = 6000)
predicted_class <- nnetPred(X, nnet.model)
print(paste('training accuracy:', mean(predicted_class == (y))))
hs <- 0.01
grid <- as.matrix(expand.grid(seq(x_min, x_max, by = hs),
seq(y_min, y_max, by = hs)))
Z <- nnetPred(grid, nnet.model)
Z
ggplot() +  geom_tile(aes(x = grid[ , 1], y = grid[,2], fill=as.character(Z)),
alpha = 0.3, show.legend = F) +
geom_point(data = data, aes(x=x, y=y, color = as.character(label)), size = 2) +
theme_bw(base_size = 15) +  ggtitle('Neural Network Decision Boundary') +
coord_fixed(ratio = 0.8) +
theme(axis.ticks=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_blank(),
axis.title=element_blank(),
legend.position = 'none')
displayDigit <- function(X) {
m <- matrix(unlist(X), nrow = 28, byrow = T)
m <- t(apply(m, 2, rev))
image(m,col=grey.colors(255))
}
train <- read.csv("data/train.csv", header = TRUE, stringsAsFactors = F)
setwd("~/Dropbox/current/NNetFromScratchUsingR/")
setwd("~/Dropbox/current/NNetFromScratchUsingR/")
train <- read.csv("data/train.csv", header = TRUE, stringsAsFactors = F)
displayDigit(train[18,-1])
nzv <- nearZeroVar(train)
nzv.nolabel <- nzv-1
inTrain <- createDataPartition(y = train$label, p = 0.7, list = F)
training <- train[inTrain, ]
CV <- train[-inTrain, ]
X <- as.matrix(training[, -1]) # data matrix (each row = single example)
N <- nrow(X) # number of examples
y <- training[, 1] # class labels
K <- length(unique(y)) # number of classes
X.proc <- X[, -nzv.nolabel]/max(X) # scale
D <- ncol(X.proc) # dimensionality X
cv <- as.matrix(CV[, -1]) # data matrix (each row = single example)
ycv <- CV[, 1] # class labels
Xcv.proc <- Xcv[, -nzv.nolabel]/max(X) # scale CV data
Y <- matrix(0, N, K)
for (i in 1:N) {
Y[i, y[i]+1] <- 1
}
#Model training and CV accuracy
# Now we can train the model with the training set. Note even after removing nzv columns, the data is still huge, so it may take a while for result to converge. Here I am only training the model for 3500 interations. You can vary the iterations, learning rate and regularization strength and plot the learning curve for optimal fitting.
nnet.mnist <- nnet(X.proc, Y, step_size = 0.3,
reg = 0.0001,
niteration = 500)
predicted_class <- nnetPred(X.proc, nnet.mnist)
print(paste('training set accuracy: ', mean(predicted_class == (y+1))))
predicted_class <- nnetPred(Xcv.proc, nnet.mnist)
print(paste('training set accuracy: ', mean(predicted_class == (y+1))))
predicted_class <- nnetPred(Xcv.proc, nnet.mnist)
ycv <- CV[, 1] # class labels
Xcv.proc <- Xcv[, -nzv.nolabel]/max(X) # scale CV data
Y <- matrix(0, N, K)
Xcv <- as.matrix(CV[, -1])
ycv <- CV[, 1] # class labels
Xcv.proc <- Xcv[, -nzv.nolabel]/max(X) # scale CV data
Y <- matrix(0, N, K)
for (i in 1:N) {
Y[i, y[i]+1] <- 1
}
predicted_class <- nnetPred(X.proc, nnet.mnist)
print(paste('training set accuracy: ', mean(predicted_class == (y+1))))
predicted_class <- nnetPred(Xcv.proc, nnet.mnist)
print(paste('CV accuracy: ', mean(predicted_class == (ycv+1))))
Xtest <- Xcv[sample(1:nrow(Xcv), 1), ]
Xtest.proc <- as.matrix(Xtest[-nzv.nolabel], nrow = 1)
predicted_test <- nnetPred(t(Xtest.proc), nnet.mnist)
print(paste('The predicted digit is: ',predicted_test-1 ))
displayDigit(Xtest)
plot(nnet.mnist)
?nnet.mnist
print(paste('training set accuracy:', mean(predicted_class == (y+1))))
print(paste('training set accuracy:', mean(predicted_class == (y+1))))
